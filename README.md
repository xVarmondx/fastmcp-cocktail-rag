# ğŸ¸ Asystent Koktajli RAG (FastMCP + LMStudio)

<p align="center">
Â  <strong>Zaawansowany system RAG (Retrieval-Augmented Generation) do serwowania precyzyjnych przepisÃ³w na koktajle dla modeli LLM.</strong>
</p>

<p align="center">
<a href="https://github.com/modelcontext/fastmcp">
Â  Â  <img src="https://img.shields.io/badge/FastMCP-2.13-blue?style=for-the-badge&logo=python&logoColor=white" alt="FastMCP">
</a>
<a href="https://lmstudio.ai/">
Â  Â  <img src="https://img.shields.io/badge/LM_Studio-0.2-blueviolet?style=for-the-badge&logo=lm-studio" alt="LM Studio">
</a>
<a href="https://www.python.org/">
Â  Â  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python" alt="Python">
</a>
</p>

## 1. O Projekcie

Ten projekt to implementacja architektury **RAG (Retrieval-Augmented Generation)**. Jego celem jest stworzenie zewnÄ™trznego systemu "narzÄ™dzi" (tools), ktÃ³ry Å‚Ä…czy model jÄ™zykowy (LLM) z dedykowanÄ…, statycznÄ… bazÄ… wiedzy.

Zamiast polegaÄ‡ na ogÃ³lnej, wewnÄ™trznej wiedzy modelu, system ten zapewnia, Å¼e odpowiedzi sÄ… **weryfikowalne** i **oparte na faktach** z dostarczonego zbioru danych.

Projekt ten demonstruje:
* ImplementacjÄ™ serwera **MCP (Model Context Protocol)** przy uÅ¼yciu **FastMCP**.
* BudowÄ™ niestandardowego silnika wyszukiwania (retrievera) w Pythonie, zdolnego do obsÅ‚ugi niekonsekwentnych danych.
* IntegracjÄ™ serwera RAG z klientem **LMStudio**, aby udostÄ™pniÄ‡ modelowi LLM (np. `qwen2`) nowe, dynamiczne moÅ¼liwoÅ›ci.

<br>

## 2. Kluczowe Technologie (WyjaÅ›nienie Koncepcji)

Zanim przejdziemy do architektury, warto wyjaÅ›niÄ‡ trzy kluczowe technologie, na ktÃ³rych opiera siÄ™ ten projekt.

### ğŸ§  Czym jest RAG (Retrieval-Augmented Generation)?

**RAG** to skrÃ³t od **Retrieval-Augmented Generation**, co moÅ¼na przetÅ‚umaczyÄ‡ jako "Generowanie Wzbogacone o Wyszukiwanie".

* **Problem:** Modele LLM (jak Qwen2 czy Llama) czÄ™sto "halucynujÄ…" lub zmyÅ›lajÄ… odpowiedzi, gdy nie znajÄ… faktÃ³w. Ich wiedza jest ograniczona do danych, na ktÃ³rych je trenowano.
* **RozwiÄ…zanie (RAG):** Zamiast polegaÄ‡ na pamiÄ™ci modelu, RAG daje mu "podrÄ™cznik" (w naszym przypadku plik `cocktail_dataset.json`) i kaÅ¼e mu z niego korzystaÄ‡ za kaÅ¼dym razem, gdy odpowiada na pytanie.

To dziaÅ‚a jak **egzamin z otwartÄ… ksiÄ…Å¼kÄ…**:
1.  **Retrieval (Wyszukiwanie):** UÅ¼ytkownik pyta o przepis. Nasz kod (`rag_engine.py`) najpierw **wyszukuje** (pobiera) prawdziwy przepis z naszego pliku JSON.
2.  **Augmentation (Wzbogacanie):** System "wzbogaca" kontekst modelu, dodajÄ…c do jego polecenia znaleziony przepis.
3.  **Generation (Generowanie):** Model LLM dostaje proste polecenie: "Na podstawie *tych* danych, ktÃ³re ci daÅ‚em, wygeneruj Å‚adnÄ… odpowiedÅº dla uÅ¼ytkownika".

DziÄ™ki temu model nie zmyÅ›la, lecz opiera siÄ™ na faktach.

### ğŸ“ Czym jest MCP (Model Context Protocol)?

**MCP** to "jÄ™zyk" lub "linia telefoniczna", ktÃ³ra pozwala modelowi LLM rozmawiaÄ‡ z naszym kodem w Pythonie.

* **Problem:** Model LLM (dziaÅ‚ajÄ…cy w LMStudio) i nasz silnik wyszukiwania (`rag_engine.py`) to dwa oddzielne programy. MuszÄ… mieÄ‡ sposÃ³b, by siÄ™ ze sobÄ… komunikowaÄ‡.
* **RozwiÄ…zanie (MCP):** MCP to protokÃ³Å‚, ktÃ³ry standaryzuje tÄ™ komunikacjÄ™.
    * Nasz `server.py` (napisany przy uÅ¼yciu **FastMCP**) dziaÅ‚a jak "kuchnia" lub centrala telefoniczna, ktÃ³ra czeka na zamÃ³wienia na porcie 8001.
    * Gdy model LLM chce coÅ› znaleÅºÄ‡, uÅ¼ywa MCP, by "zadzwoniÄ‡" do naszego serwera i zÅ‚oÅ¼yÄ‡ "zamÃ³wienie" (np. "proszÄ™, uÅ¼yj narzÄ™dzia `get_cocktail_recipe` dla 'Mojito'").
    * Nasz serwer odbiera to, uruchamia `rag_engine.py` i odsyÅ‚a dane.

### ğŸ–¥ï¸ Czym jest LMStudio?

**LMStudio** to darmowa aplikacja na komputery stacjonarne, ktÃ³ra pozwala kaÅ¼demu pobieraÄ‡ i uruchamiaÄ‡ potÄ™Å¼ne modele LLM (jak te od Mety, Google czy Mistral) lokalnie, na wÅ‚asnym komputerze.

W naszym projekcie LMStudio peÅ‚ni **dwie kluczowe role**:
1.  **Host Modelu:** Jest "domem" dla modelu LLM (np. `qwen2`), ktÃ³ry jest "mÃ³zgiem" caÅ‚ej operacji.
2.  **Klient MCP:** DziaÅ‚a jako "telefon", ktÃ³ry uÅ¼ywa protokoÅ‚u MCP do Å‚Ä…czenia siÄ™ z serwerem i korzystania z narzÄ™dzi, ktÃ³re mu udostÄ™pniliÅ›my.

<br >

## 3. Architektura Systemu (Diagram PrzepÅ‚ywu)

PoniÅ¼szy diagram ilustruje, jak informacja przepÅ‚ywa przez system, od zapytania uÅ¼ytkownika do finalnej odpowiedzi.

```mermaid
graph TD
    subgraph "Klient (LMStudio)"
        A[UÅ¼ytkownik] -- 1. Prompt: 'I have Gin and Lemon Peel' --> B(Model LLM - Qwen2);
        B -- 8. Finalna odpowiedÅº (wygenerowana z JSON) --> A;
    end

    subgraph "Serwer (Python @ Port 8001)"
        C{{FastMCP Server - server.py}};
        D[Silnik RAG - rag_engine.py];
        E[(Baza Danych - cocktail_dataset.json)];
    end

    B -- 2. Å»Ä…danie MCP: WywoÅ‚aj 'suggest_cocktails(...)' --> C;
    C -- 3. WywoÅ‚anie funkcji: find_cocktails_by_ingredients(...) --> D;
    D -- 4. Odczyt i parsowanie danych --> E;
    E -- 5. Zwraca surowe dane [JSON Array] --> D;
    D -- 6. Zwraca zÅ‚oÅ¼ony obiekt Python: {'perfect': [...], 'partial': [...]} --> C;
    C -- 7. 'SpÅ‚aszcza' dane i zwraca prosty JSON (Tool Result) --> B;
```

```mermaid
graph LR
    subgraph "Krok 1: UÅ¼ytkownik (LMStudio)"
        A[Zapytanie UÅ¼ytkownika<br/>'I have Gin and Lemon Peel']
    end

    subgraph "Krok 2: Model LLM (LMStudio)"
        B("WywoÅ‚anie NarzÄ™dzia (JSON)<br/>(Argumenty wygenerowane przez LLM)<br/><br/><strong>Payload:</strong><br/>['Gin', 'Lemon Peel']")
    end

    subgraph "Krok 3: Normalizacja Danych (rag_engine.py)"
        C("Funkcja: _normalize_ingredient<br/>(Czyszczenie 'brudnych' danych)<br/><br/><strong>Payload:</strong><br/>{'gin', 'lemon peel'}")
    end

    subgraph "Krok 4: Wyszukiwanie (rag_engine.py)"
        D("ZÅ‚oÅ¼ony Wynik (Python Dict)<br/>(Zwracany do server.py)<br/><br/><strong>Payload:</strong><br/>{<br/>&nbsp;&nbsp;'perfect': [ { 'cocktail': {...} }, { 'cocktail': {...} } ],<br/>&nbsp;&nbsp;'partial': [ { 'cocktail': {...} } ]<br/>}")
    end

    subgraph "Krok 5: SpÅ‚aszczanie (server.py)"
        E("Prosty Wynik (JSON)<br/>(WysyÅ‚any z powrotem do LLM)<br/><br/><strong>Tabela: perfect_matches</strong><br/>| name | matched_ingredients |<br/>| 'Alaska Cocktail' | ['gin', 'lemon peel'] |<br/>| 'Gin Toddy' | ['gin', 'lemon peel'] |<br/>| ... (Limit do 5) ... |<br/><br/><strong>Tabela: partial_matches</strong><br/>| name | missing_count |<br/>| 'Negroni' | 1 |<br/>| ... (Limit do 5) ... |")
    end

    subgraph "Krok 6: Generowanie (LMStudio)"
        F(Model LLM generuje odpowiedÅº<br/>czytajÄ…c **tylko** prosty JSON z Kroku 5)
    end

    A -- Zapytanie --> B;
    B -- WywoÅ‚anie narzÄ™dzia --> C;
    C -- Przetwarzanie --> D;
    D -- Transformacja --> E;
    E -- Wynik narzÄ™dzia --> F;
```

```json
{
  "ingredients": ["Gin", "Lemon Peel"]
}
