# ğŸ¸ Asystent Koktajli RAG (FastMCP + LMStudio)

<p align="center">
Â  <strong>Zaawansowany system RAG (Retrieval-Augmented Generation) do serwowania precyzyjnych przepisÃ³w na koktajle dla modeli LLM.</strong>
</p>

<p align="center">
<a href="https://github.com/modelcontext/fastmcp">
Â  Â  <img src="https://img.shields.io/badge/FastMCP-2.13-blue?style=for-the-badge&logo=python&logoColor=white" alt="FastMCP">
</a>
<a href="https://lmstudio.ai/">
Â  Â  <img src="https://img.shields.io/badge/LM_Studio-0.2-blueviolet?style=for-the-badge&logo=lm-studio" alt="LM Studio">
</a>
<a href="https://www.python.org/">
Â  Â  <img src="https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python" alt="Python">
</a>
</p>

## 1. O Projekcie

Ten projekt to implementacja architektury **RAG (Retrieval-Augmented Generation)**. Jego celem jest stworzenie zewnÄ™trznego systemu "narzÄ™dzi" (tools), ktÃ³ry Å‚Ä…czy model jÄ™zykowy (LLM) z dedykowanÄ…, statycznÄ… bazÄ… wiedzy.

Zamiast polegaÄ‡ na ogÃ³lnej, wewnÄ™trznej wiedzy modelu, system ten zapewnia, Å¼e odpowiedzi sÄ… **weryfikowalne** i **oparte na faktach** z dostarczonego zbioru danych.

Projekt ten demonstruje:
* ImplementacjÄ™ serwera **MCP (Model Context Protocol)** przy uÅ¼yciu **FastMCP**.
* BudowÄ™ niestandardowego silnika wyszukiwania (retrievera) w Pythonie, zdolnego do obsÅ‚ugi niekonsekwentnych danych.
* IntegracjÄ™ serwera RAG z klientem **LMStudio**, aby udostÄ™pniÄ‡ modelowi LLM (np. `qwen2`) nowe, dynamiczne moÅ¼liwoÅ›ci.

<br>

## 2. Kluczowe Technologie (WyjaÅ›nienie Koncepcji)

Zanim przejdziemy do architektury, warto wyjaÅ›niÄ‡ trzy kluczowe technologie, na ktÃ³rych opiera siÄ™ ten projekt.

### ğŸ§  Czym jest RAG (Retrieval-Augmented Generation)?

**RAG** to skrÃ³t od **Retrieval-Augmented Generation**, co moÅ¼na przetÅ‚umaczyÄ‡ jako "Generowanie Wzbogacone o Wyszukiwanie".

* **Problem:** Modele LLM (jak Qwen2 czy Llama) czÄ™sto "halucynujÄ…" lub zmyÅ›lajÄ… odpowiedzi, gdy nie znajÄ… faktÃ³w. Ich wiedza jest ograniczona do danych, na ktÃ³rych je trenowano.
* **RozwiÄ…zanie (RAG):** Zamiast polegaÄ‡ na pamiÄ™ci modelu, RAG daje mu "podrÄ™cznik" (w naszym przypadku plik `cocktail_dataset.json`) i kaÅ¼e mu z niego korzystaÄ‡ za kaÅ¼dym razem, gdy odpowiada na pytanie.

To dziaÅ‚a jak **egzamin z otwartÄ… ksiÄ…Å¼kÄ…**:
1.  **Retrieval (Wyszukiwanie):** UÅ¼ytkownik pyta o przepis. Nasz kod (`rag_engine.py`) najpierw **wyszukuje** (pobiera) prawdziwy przepis z naszego pliku JSON.
2.  **Augmentation (Wzbogacanie):** System "wzbogaca" kontekst modelu, dodajÄ…c do jego polecenia znaleziony przepis.
3.  **Generation (Generowanie):** Model LLM dostaje proste polecenie: "Na podstawie *tych* danych, ktÃ³re ci daÅ‚em, wygeneruj Å‚adnÄ… odpowiedÅº dla uÅ¼ytkownika".

DziÄ™ki temu model nie zmyÅ›la, lecz opiera siÄ™ na faktach.

### ğŸ“ Czym jest MCP (Model Context Protocol)?

**MCP** to "jÄ™zyk" lub "linia telefoniczna", ktÃ³ra pozwala modelowi LLM rozmawiaÄ‡ z naszym kodem w Pythonie.

* **Problem:** Model LLM (dziaÅ‚ajÄ…cy w LMStudio) i nasz silnik wyszukiwania (`rag_engine.py`) to dwa oddzielne programy. MuszÄ… mieÄ‡ sposÃ³b, by siÄ™ ze sobÄ… komunikowaÄ‡.
* **RozwiÄ…zanie (MCP):** MCP to protokÃ³Å‚, ktÃ³ry standaryzuje tÄ™ komunikacjÄ™.
    * Nasz `server.py` (napisany przy uÅ¼yciu **FastMCP**) dziaÅ‚a jak "kuchnia" lub centrala telefoniczna, ktÃ³ra czeka na zamÃ³wienia na porcie 8001.
    * Gdy model LLM chce coÅ› znaleÅºÄ‡, uÅ¼ywa MCP, by "zadzwoniÄ‡" do naszego serwera i zÅ‚oÅ¼yÄ‡ "zamÃ³wienie" (np. "proszÄ™, uÅ¼yj narzÄ™dzia `get_cocktail_recipe` dla 'Mojito'").
    * Nasz serwer odbiera to, uruchamia `rag_engine.py` i odsyÅ‚a dane.

### ğŸ–¥ï¸ Czym jest LMStudio?

**LMStudio** to darmowa aplikacja na komputery stacjonarne, ktÃ³ra pozwala kaÅ¼demu pobieraÄ‡ i uruchamiaÄ‡ potÄ™Å¼ne modele LLM (jak te od Mety, Google czy Mistral) lokalnie, na wÅ‚asnym komputerze.

W naszym projekcie LMStudio peÅ‚ni **dwie kluczowe role**:
1.  **Host Modelu:** Jest "domem" dla modelu LLM (np. `qwen2`), ktÃ³ry jest "mÃ³zgiem" caÅ‚ej operacji.
2.  **Klient MCP:** DziaÅ‚a jako "telefon", ktÃ³ry uÅ¼ywa protokoÅ‚u MCP do Å‚Ä…czenia siÄ™ z serwerem i korzystania z narzÄ™dzi, ktÃ³re mu udostÄ™pniliÅ›my.

<br >

## 3. Architektura Systemu (Diagram PrzepÅ‚ywu)

PoniÅ¼szy diagram ilustruje, jak informacja przepÅ‚ywa przez system, od zapytania uÅ¼ytkownika do finalnej odpowiedzi.

```mermaid
graph TD
    subgraph "Klient (LMStudio)"
        A[UÅ¼ytkownik] -- 1. Prompt: 'I have Gin and Lemon Peel' --> B(Model LLM - Qwen2);
        B -- 8. Finalna odpowiedÅº (wygenerowana z JSON) --> A;
    end

    subgraph "Serwer (Python @ Port 8001)"
        C{{FastMCP Server - server.py}};
        D[Silnik RAG - rag_engine.py];
        E[(Baza Danych - cocktail_dataset.json)];
    end

    B -- 2. Å»Ä…danie MCP: WywoÅ‚aj 'suggest_cocktails(...)' --> C;
    C -- 3. WywoÅ‚anie funkcji: find_cocktails_by_ingredients(...) --> D;
    D -- 4. Odczyt i parsowanie danych --> E;
    E -- 5. Zwraca surowe dane [JSON Array] --> D;
    D -- 6. Zwraca zÅ‚oÅ¼ony obiekt Python: {'perfect': [...], 'partial': [...]} --> C;
    C -- 7. 'SpÅ‚aszcza' dane i zwraca prosty JSON (Tool Result) --> B;
```

```mermaid
graph LR
    subgraph "Krok 1: UÅ¼ytkownik (LMStudio)"
        A[Zapytanie UÅ¼ytkownika<br/>'I have Gin and Lemon Peel']
    end

    subgraph "Krok 2: Model LLM (LMStudio)"
        B("WywoÅ‚anie NarzÄ™dzia (JSON)<br/>(Argumenty wygenerowane przez LLM)<br/><br/><strong>Payload:</strong><br/>['Gin', 'Lemon Peel']")
    end

    subgraph "Krok 3: Normalizacja Danych (rag_engine.py)"
        C("Funkcja: _normalize_ingredient<br/>(Czyszczenie 'brudnych' danych)<br/><br/><strong>Payload:</strong><br/>{'gin', 'lemon peel'}")
    end

    subgraph "Krok 4: Wyszukiwanie (rag_engine.py)"
        D("ZÅ‚oÅ¼ony Wynik (Python Dict)<br/>(Zwracany do server.py)<br/><br/><strong>Payload:</strong><br/>{<br/>&nbsp;&nbsp;'perfect': [ { 'cocktail': {...} }, { 'cocktail': {...} } ],<br/>&nbsp;&nbsp;'partial': [ { 'cocktail': {...} } ]<br/>}")
    end

    subgraph "Krok 5: SpÅ‚aszczanie (server.py)"
        E("Prosty Wynik (JSON)<br/>(WysyÅ‚any z powrotem do LLM)<br/><br/><strong>Tabela: perfect_matches</strong><br/>| name | matched_ingredients |<br/>| 'Alaska Cocktail' | ['gin', 'lemon peel'] |<br/>| 'Gin Toddy' | ['gin', 'lemon peel'] |<br/>| ... (Limit do 5) ... |<br/><br/><strong>Tabela: partial_matches</strong><br/>| name | missing_count |<br/>| 'Negroni' | 1 |<br/>| ... (Limit do 5) ... |")
    end

    subgraph "Krok 6: Generowanie (LMStudio)"
        F(Model LLM generuje odpowiedÅº<br/>czytajÄ…c **tylko** prosty JSON z Kroku 5)
    end

    A -- Zapytanie --> B;
    B -- WywoÅ‚anie narzÄ™dzia --> C;
    C -- Przetwarzanie --> D;
    D -- Transformacja --> E;
    E -- Wynik narzÄ™dzia --> F;
```

## 4. Architektura Danych: PodrÃ³Å¼ Jednego Zapytania

Aby najlepiej zrozumieÄ‡, jak system przetwarza dane, przeÅ›ledÅºmy podrÃ³Å¼ zapytania **"I have Gin and Lemon Peel"** przez caÅ‚Ä… architekturÄ™.

### Krok 1: Zapytanie UÅ¼ytkownika (Surowy Tekst)
UÅ¼ytkownik wpisuje w LMStudio:

```
I have Gin and Lemon Peel
```

### Krok 2: WywoÅ‚anie NarzÄ™dzia (JSON od LLM do Serwera)
Model LLM (`qwen2`) rozpoznaje, Å¼e musi uÅ¼yÄ‡ narzÄ™dzia. Parsuje zapytanie uÅ¼ytkownika do formatu JSON i wysyÅ‚a je do naszego serwera `server.py`:
```json
{
  "ingredients": ["Gin", "Lemon Peel"]
}
```

### Krok 3: Normalizacja Danych (w rag_engine.py)
Nasz silnik RAG odbiera ten JSON. Funkcja (`_normalize_ingredient`) natychmiast czyÅ›ci dane, aby poradziÄ‡ sobie z "brudnÄ…" bazÄ… danych

| Dane WejÅ›ciowe (z LLM) | Po Normalizacji (w Pythonie) | Uzasadnienie |
|:---|:---|:---|
| "Gin" | "gin" | Standardyzacja (maÅ‚e litery) |
| "Lemon Peel" | "lemon peel" | **Kluczowy krok:** RozrÃ³Å¼nienie od soku ("lemon") |


Wynikowy zestaw do wyszukania: (`{'gin', 'lemon peel'}`)

### Krok 4: Wyszukiwanie i "SpÅ‚aszczanie" (w server.py)
Nasz rag_engine.py znajduje wszystkie pasujÄ…ce koktajle i zwraca je do server.py jako zÅ‚oÅ¼ony obiekt. server.py nastÄ™pnie spÅ‚aszcza te dane, aby przygotowaÄ‡ prostÄ… odpowiedÅº dla modelu LLM.

### Krok 5: Wynik NarzÄ™dzia (Finalny JSON wysÅ‚any do LLM)
Model LLM nie otrzymuje skomplikowanego, zagnieÅ¼dÅ¼onego obiektu. Zamiast tego, server.py wysyÅ‚a mu ten prosty, "spÅ‚aszczony" JSON, ktÃ³ry jest Å‚atwy do odczytania:

```json
{
  "status": "success",
  "type": "suggestion_by_ingredient",
  "perfect_matches": [ ... ],
  "partial_matches": [ ... ]
}
```

Kluczowe listy (`perfect_matches`) i (`partial_matches`) wewnÄ…trz tego JSON-a moÅ¼na zwizualizowaÄ‡ jako te tabele:


Tabela: perfect_matches (Dane wysÅ‚ane do LLM)
(Znaleziono koktajle, ktÃ³re majÄ… oba skÅ‚adniki: "gin" i "lemon peel")

| Nazwa (name) | Dopasowane SkÅ‚adniki (matched ingredients) | PeÅ‚na Lista SkÅ‚adnikÃ³w (all ingredients in recipe) |
|:-------------|:------------------------------------------|:---------------------------------------------------|
| Alaska | ['gin', 'lemon peel'] | ["1 1/2 oz Gin", "Twist of Lemon Peel", ...] |
| Gin Toddy | ['gin', 'lemon peel'] | ["2 oz Gin", "1 twist of Lemon Peel", ...] |
| Bermuda | ['gin', 'lemon peel'] | ["3/4 oz Gin", "3/4 oz Brandy", "Lemon Peel", ...] |
| ... (Limit do 5) | ... | ... |

Tabela: partial_matches (Dane wysÅ‚ane do LLM)
(Znaleziono koktajle, ktÃ³re majÄ… tylko jeden ze skÅ‚adnikÃ³w)

| Nazwa (name) | Dopasowane SkÅ‚adniki (matched_ingredients) | Liczba BrakujÄ…cych SkÅ‚adnikÃ³w (missing_ingredients_count) |
|:-------------|:------------------------------------------|:---------------------------------------------------------:|
| Negroni | ['gin'] | 1 |
| Gin And Tonic | ['gin'] | 1 |
| Whiskey Sour | ['lemon'] | 1 |
| ... (Limit do 5) | ... | ... |



### Krok 6: Generowanie Odpowiedzi (Surowy Tekst)
Model LLM otrzymuje proste, tabelaryczne dane z Kroku 5. Zgodnie z System Promptem, jego zadaniem jest tylko odczytanie ich i zaprezentowanie:
```
"Based on the ingredients you have, here are some cocktail suggestions:

Alaska Cocktail (Matched Ingredients: gin, lemon peel)

Gin Toddy (Matched Ingredients: gin, lemon peel)

...

You can also consider the following partial matches (missing 1 ingredient):

Negroni"
```


---

## 5. Instrukcja Uruchomienia (Krok po Kroku)

### Krok 1: Pobranie i Instalacja

Aby uruchomiÄ‡ projekt, najpierw sklonuj to repozytorium na swÃ³j lokalny komputer i zainstaluj wymagane zaleÅ¼noÅ›ci.

```bash
# 1. Sklonuj repozytorium
git clone https://github.com/xVarmondx/fastmcp-cocktail-rag

# 2. WejdÅº do folderu projektu
cd [NAZWA_FOLDERU_PROJEKTU]

# 3. (Zalecane) StwÃ³rz wirtualne Å›rodowisko
python -m venv .venv

# 4. Aktywuj wirtualne Å›rodowisko
#    Na Windows:
.venv\Scripts\activate
#    Na macOS/Linux:
source .venv/bin/activate

# 5. Zainstaluj zaleÅ¼noÅ›ci
pip install "fastmcp[http]"
```

### Krok 2: Uruchomienie Serwera RAG (Python)

Upewnij siÄ™, Å¼e TwÃ³j `cocktail_dataset.json` znajduje siÄ™ w Å›cieÅ¼ce okreÅ›lonej w `server.py` (domyÅ›lnie: `dataset/cocktail_dataset.json`).

W terminalu, w ktÃ³rym masz aktywowane Å›rodowisko `.venv`, uruchom serwer:

```bash
python server.py
```

Serwer uruchomi siÄ™ na porcie 8001. JeÅ›li wszystko poszÅ‚o dobrze, powinieneÅ› zobaczyÄ‡ w konsoli potwierdzenie zaÅ‚adowania bazy danych:

```
Successfully loaded 134 cocktails from dataset/cocktail_dataset.json
Starting FastMCP Cocktail RAG server...
...
Server started.
```

Nie zamykaj tego terminala.


### Krok 3: Konfiguracja Klienta (LMStudio)
Teraz, gdy nasz serwer RAG dziaÅ‚a, musimy skonfigurowaÄ‡ LMStudio, aby z nim rozmawiaÅ‚o.

1.Uruchom LMStudio.

2.PrzejdÅº do zakÅ‚adki Discover (Lupa) na pasku po lewej stronie.

3. Wyszukaj i pobierz model : (`qwen2-vl-7b-instruct`) (7.39GB)

4. WejdÅº w zakÅ‚adkÄ™ Chat po lewej stronie, nastÄ™pnie po prawej stronie wybierz zakÅ‚adkÄ™ Program->Install->Edit mcp.json
Ustaw poÅ‚Ä…czenie z serwerem

```json
{
  "mcpServers": {
    "CocktailRAGAssistant": {
      "url": "http://127.0.0.1:8001/mcp"
    }
  }
}
```

I kliknij (`Save`)

5. WejdÅº do zakÅ‚adki (`Developer`) i uruchom serwer czatowy LMStudio, klikajÄ…c Start Server u gÃ³ry.
6. WejdÅº do zakÅ‚adki (`Chat`) i dodaj nowy chat (+) i wpisz zapytanie:

---

## 6. PrzykÅ‚adowe Testy (Jak sprawdziÄ‡, czy dziaÅ‚a)

Po wykonaniu wszystkich powyÅ¼szych krokÃ³w, system jest gotowy do pracy. Oto zestaw zapytaÅ„ testowych, ktÃ³re moÅ¼esz zadaÄ‡ w LMStudio, aby sprawdziÄ‡ kaÅ¼dÄ… z trzech gÅ‚Ã³wnych funkcjonalnoÅ›ci RAG.

### A. Test: Pytania o koktajle i ich skÅ‚adniki
*NarzÄ™dzie: `get_cocktail_recipe`*

Te prompty sprawdzajÄ…, czy system potrafi znaleÅºÄ‡ konkretny przepis w bazie danych `cocktail_dataset.json`.

**Test 1: Zapytanie o przepis (Sukces)**
```
What is the recipe for an Apricot Lady?
```

* **Oczekiwany wynik:** Model poprawnie wywoÅ‚a narzÄ™dzie i zwrÃ³ci przepis na `Apricot Lady`.

**Test 2: Zapytanie o przepis ktÃ³rego nie ma w bazie**

```
I'd like the recipe for a Cosmopolitan.
```

* **Oczekiwany wynik:** Model poprawnie stwierdzi, Å¼e nie znalazÅ‚ przepisu (poniewaÅ¼ nie ma go w pliku JSON), zamiast go wymyÅ›liÄ‡.

---

### B. Test: Sugerowanie na podstawie skÅ‚adnikÃ³w
*NarzÄ™dzie: `suggest_cocktails_by_ingredients`*

Te prompty sprawdzajÄ… logikÄ™ `_normalize_ingredient` w `rag_engine.py` i zdolnoÅ›Ä‡ systemu do radzenia sobie z niekonsekwentnymi danymi.

**Test 3: Grupowanie SynonimÃ³w (Lemon vs Lemon Juice)**

```
I have Gin and Lemon Juice. What can I make?
```

* **Oczekiwany wynik:** System powinien znaleÅºÄ‡ koktajle zarÃ³wno z `"lemon"` (np. `Long Island Tea`) jak i `"lemon juice"` (np. `Gin Sour`).

**Test 4: RozrÃ³Å¼nianie SkÅ‚adnikÃ³w (Lemon vs Lemon Peel)**

```
I have Gin and Lemon Peel. What can I make?
```

* **Oczekiwany wynik:** System **NIE powinien** pokazywaÄ‡ `Long Island Tea`. Powinien poprawnie zwrÃ³ciÄ‡ koktajle, ktÃ³re faktycznie zawierajÄ… `"Lemon Peel"` (np. `Alaska Cocktail` lub `Gin Toddy`).

---

### C. Test: Sugerowanie na podstawie preferencji smakowych
*NarzÄ™dzie: `suggest_cocktails_by_preference`*

Te prompty sprawdzajÄ… filtrowanie po tagach.

**Test 5: Filtrowanie po tagach (Sukces - Logika "AND")**

```
Suggest a cocktail that is "IBA" and "Classic".
```

* **Oczekiwany wynik:** Model pokaÅ¼e listÄ™ zawierajÄ…cÄ… m.in. `Old Fashioned`, `Negroni` i `Dry Martini`.

**Test 6: Filtrowanie po tagach (ObsÅ‚uga bÅ‚Ä™dÃ³w / `null`)**

```
I want something that is "Vegan" and "Savory".
```
* **Oczekiwany wynik:** System nie ulegnie awarii (dziÄ™ki obsÅ‚udze tagÃ³w `null`). Model poprawnie stwierdzi, Å¼e nic nie znalazÅ‚, poniewaÅ¼ Å¼aden koktajl w bazie nie ma obu tych tagÃ³w jednoczeÅ›nie (`Mojito` jest "Vegan", `Old Fashioned` jest "Savory").





















